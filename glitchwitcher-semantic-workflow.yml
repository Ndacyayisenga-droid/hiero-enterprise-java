# This workflow has been improved to ensure:
# 1. The same CSV dataset generated is used for training
# 2. Trained models are committed to the same PR as the dataset
# 3. Both dataset and models are used consistently for analysis
# 4. Scripts are downloaded from external repository

name: GlitchWitcher - Semantic Bug Prediction Analysis

on:
  issue_comment:
    types: [created]

permissions:
  pull-requests: write
  issues: write
  contents: write

jobs:
  glitch-witcher-semantic:
    runs-on: ubuntu-latest
    if: contains(github.event.comment.body, 'GlitchWitcher-Semantic')

    steps:
      - name: Parse GlitchWitcher-Semantic Command
        id: parse-command
        uses: actions/github-script@v6
        with:
          script: |
            const body = context.payload.comment?.body ?? '';
            core.info(`Full comment: ${body}`);
            // Look for an explicit PR link in the comment
            const linkMatch = body.match(/https:\/\/github\.com\/[^/]+\/[^/]+\/pull\/\d+/);
            let prLink = null;
            if (linkMatch) {
              prLink = linkMatch[0];
              core.info(`PR link provided: ${prLink}`);
            } else {
              // Allow "GlitchWitcher-Semantic" alone when the comment is on a PR
              const hasCmdOnly = /(^|\s)GlitchWitcher-Semantic\s*$/.test(body);
              if (hasCmdOnly && context.payload.issue?.pull_request) {
                const { owner, repo } = context.repo;
                const prNumber = context.issue.number;
                prLink = `https://github.com/${owner}/${repo}/pull/${prNumber}`;
                core.info(`Using current PR: ${prLink}`);
              } else {
                core.setFailed('ERROR: Invalid GlitchWitcher-Semantic command format or missing PR link');
                return;
              }
            }
            // Extract repo owner/name/number from the PR link
            const m = prLink.match(/^https:\/\/github\.com\/([^/]+)\/([^/]+)\/pull\/(\d+)$/);
            if (!m) {
              core.setFailed(`ERROR: Could not parse repository info from PR link: ${prLink}`);
              return;
            }
            const [, repoOwner, repoName, prNumber] = m;
            const fullRepoName = `${repoOwner}-${repoName}`;
            const repoUrl = `https://github.com/${repoOwner}/${repoName}.git`;
            core.setOutput('repo_owner', repoOwner);
            core.setOutput('repo_name', repoName);
            core.setOutput('pr_number', prNumber);
            core.setOutput('full_repo_name', fullRepoName);
            core.setOutput('pr_link', prLink);
            core.setOutput('repo_url', repoUrl)

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          pip install tensorflow==2.12.0 pandas joblib scipy numpy urllib3 scikit-learn javalang torch keras
          sudo apt-get update
          sudo apt-get install -y cloc git openjdk-11-jdk

      - name: Download Scripts from External Repository
        run: |
          echo "Downloading semantic analysis scripts from external repository..."
          
          # Function to download with error handling
          download_script() {
            local url="$1"
            local filename="$2"
            echo "Downloading $filename from $url"
            if curl -L -f -o "$filename" "$url"; then
              echo "✅ Successfully downloaded $filename"
              return 0
            else
              echo "❌ Failed to download $filename from $url"
              return 1
            fi
          }
          
          # Download scripts with error handling
          failed_downloads=()
          
          if ! download_script "https://raw.githubusercontent.com/adoptium/aqa-triage-data/refs/heads/main/GlitchWitcher/Semantic%20Dataset/extract_semantic_dataset.sh" "extract_semantic_dataset.sh"; then
            failed_downloads+=("extract_semantic_dataset.sh")
          fi
          
          if ! download_script "https://raw.githubusercontent.com/adoptium/aqa-test-tools/refs/heads/master/BugPredict/GlitchWitcher/Semantic/analyze_java_file.py" "analyze_java_file.py"; then
            failed_downloads+=("analyze_java_file.py")
          fi
          
          if ! download_script "https://raw.githubusercontent.com/adoptium/aqa-test-tools/refs/heads/master/BugPredict/GlitchWitcher/Semantic/compare_semantic_predictions.py" "compare_semantic_predictions.py"; then
            failed_downloads+=("compare_semantic_predictions.py")
          fi
          
          if ! download_script "https://raw.githubusercontent.com/adoptium/aqa-test-tools/refs/heads/master/BugPredict/GlitchWitcher/Semantic/train_semantic_model.py" "train_semantic_model.py"; then
            failed_downloads+=("train_semantic_model.py")
          fi
          
          if ! download_script "https://raw.githubusercontent.com/adoptium/aqa-test-tools/refs/heads/master/BugPredict/GlitchWitcher/Semantic/train_semantic_model_safe.py" "train_semantic_model_safe.py"; then
            failed_downloads+=("train_semantic_model_safe.py")
          fi
          
          if ! download_script "https://raw.githubusercontent.com/adoptium/aqa-test-tools/refs/heads/master/BugPredict/GlitchWitcher/Semantic/load_semantic_model.py" "load_semantic_model.py"; then
            failed_downloads+=("load_semantic_model.py")
          fi
          
          # Check for failed downloads
          if [ ${#failed_downloads[@]} -gt 0 ]; then
            echo "❌ Failed to download the following scripts:"
            printf '%s\n' "${failed_downloads[@]}"
            echo "Please check the URLs and repository structure."
            exit 1
          fi
          
          # Make shell scripts executable
          if [ -f "extract_semantic_dataset.sh" ]; then
            chmod +x extract_semantic_dataset.sh
            echo "✅ Made extract_semantic_dataset.sh executable"
          else
            echo "❌ extract_semantic_dataset.sh not found after download"
            exit 1
          fi
          
          # Verify downloads
          echo "Verifying downloaded files..."
          ls -la *.py *.sh
          echo "✅ All scripts downloaded successfully"

      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Repository
        run: |
          echo "Setting up repository for semantic analysis..."
          # Ensure we're in the right directory
          pwd
          ls -la
          echo "Checking if data directory exists:"
          ls -la data/ || echo "data directory not found"
          if [ -f "data/openj9_metrics.csv" ]; then
            echo "✅ data/openj9_metrics.csv found!"
            wc -l data/openj9_metrics.csv
          else
            echo "❌ data/openj9_metrics.csv not found"
          fi

      - name: Check Dataset Availability
        id: check-dataset
        run: |
          echo "Checking local dataset availability..."
          dataset_exists="false"
          model_exists="false"
          
          # Check if local dataset CSV exists for the target repository
          repo_name="${{ steps.parse-command.outputs.repo_name }}"
          dataset_file="data/${repo_name}_metrics.csv"
          
          if [ -f "$dataset_file" ]; then
            echo "Local dataset CSV found: $dataset_file"
            dataset_exists="true"
            dataset_path="$dataset_file"
          else
            echo "Local dataset CSV not found for $repo_name, will generate new one"
            dataset_path=""
          fi
          
          # Check if local model artifacts exist
          if [ -f "seantic_trained_models/repd_model_DA.pkl" ] && \
             [ -f "seantic_trained_models/scaler.pkl" ] && \
             [ -f "seantic_trained_models/training_results.pkl" ]; then
            echo "Local semantic model artifacts found"
            model_exists="true"
            model_path="seantic_trained_models"
          else
            echo "Local semantic model artifacts not found, will train new model"
            model_path=""
          fi
          
          echo "dataset_exists=$dataset_exists" >> $GITHUB_OUTPUT
          echo "model_exists=$model_exists" >> $GITHUB_OUTPUT
          echo "dataset_path=$dataset_path" >> $GITHUB_OUTPUT
          echo "model_path=$model_path" >> $GITHUB_OUTPUT
          echo "repo_name=$repo_name" >> $GITHUB_OUTPUT
          echo "dataset_file=$dataset_file" >> $GITHUB_OUTPUT

      - name: Ensure .gitignore for venv
        if: steps.check-dataset.outputs.dataset_exists == 'false'
        run: |
          echo "Ensuring .gitignore excludes venv/..."
          if [ ! -f .gitignore ]; then
            echo "venv/" > .gitignore
          elif ! grep -Fx "venv/" .gitignore; then
            echo "venv/" >> .gitignore
          fi
          cat .gitignore

      - name: Generate Dataset and Train Model if Missing
        if: steps.check-dataset.outputs.dataset_exists == 'false' || steps.check-dataset.outputs.model_exists == 'false'
        run: |
          echo "Generating dataset and training model for ${{ steps.check-dataset.outputs.repo_name }}..."
          echo "Repository URL: ${{ steps.parse-command.outputs.repo_url }}"
          
          # Generate dataset if missing
          if [ "${{ steps.check-dataset.outputs.dataset_exists }}" = "false" ]; then
            echo "Generating dataset..."
            # Verify the script exists and is executable
            if [ ! -f "extract_semantic_dataset.sh" ]; then
              echo "❌ ERROR: extract_semantic_dataset.sh not found!"
              echo "Available files:"
              ls -la *.sh *.py 2>/dev/null || echo "No .sh or .py files found"
              exit 1
            fi
            
            # Make the script executable and run it
            chmod +x extract_semantic_dataset.sh
            echo "Running extract_semantic_dataset.sh with URL: ${{ steps.parse-command.outputs.repo_url }}"
            ./extract_semantic_dataset.sh "${{ steps.parse-command.outputs.repo_url }}"
            
            # Find the generated CSV file
            generated_csv=$(find . -name "*_metrics.csv" -path "./metrics_output/*" | head -1)
            if [ ! -f "$generated_csv" ]; then
              echo "ERROR: Failed to generate CSV file"
              exit 1
            fi
            
            echo "Generated CSV file: $generated_csv"
            
            # Move the generated file to the data directory with the correct name
            target_file="${{ steps.check-dataset.outputs.dataset_file }}"
            mkdir -p data
            mv "$generated_csv" "$target_file"
            
            echo "Moved dataset to: $target_file"
            echo "csv_file_path=$target_file" >> $GITHUB_ENV
            dataset_path="$target_file"
          else
            echo "Using existing dataset: ${{ steps.check-dataset.outputs.dataset_path }}"
            echo "csv_file_path=${{ steps.check-dataset.outputs.dataset_path }}" >> $GITHUB_ENV
            dataset_path="${{ steps.check-dataset.outputs.dataset_path }}"
          fi
          
          # Train model using the dataset (generated or existing)
          echo "Training semantic model using dataset: $dataset_path"
          
          echo "🤖 Training semantic models..."
          
          # Try the main training script first
          if python3 train_semantic_model.py "$dataset_path"; then
            echo "✅ Main training script succeeded"
          else
            echo "⚠️ Main training script failed, trying safe alternative..."
            # Try the safe alternative training script
            if python3 train_semantic_model_safe.py "$dataset_path"; then
              echo "✅ Safe training script succeeded"
            else
              echo "❌ Both training scripts failed"
              exit 1
            fi
          fi
          
          if [ ! -d "trained_model" ]; then
            echo "ERROR: Failed to generate trained model directory"
            exit 1
          fi
          
          # Copy trained models to seantic_trained_models directory for consistency
          mkdir -p seantic_trained_models
          if [ -f "trained_model/repd_model_DA.pkl" ]; then
            cp trained_model/repd_model_DA.pkl seantic_trained_models/
          fi
          if [ -f "trained_model/scaler.pkl" ]; then
            cp trained_model/scaler.pkl seantic_trained_models/
          fi
          if [ -f "trained_model/training_results.pkl" ]; then
            cp trained_model/training_results.pkl seantic_trained_models/
          fi
          
          echo "Model training completed successfully"

      - name: Create Pull Request for Dataset and Models
        id: create-pr-dataset-models
        if: steps.check-dataset.outputs.dataset_exists == 'false' || steps.check-dataset.outputs.model_exists == 'false'
        uses: peter-evans/create-pull-request@v6
        with:
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN }}
          branch: add-dataset-models-${{ steps.check-dataset.outputs.repo_name }}
          commit-message: Add semantic dataset and trained models for ${{ steps.check-dataset.outputs.repo_name }}
          title: Add semantic dataset and trained models for ${{ steps.check-dataset.outputs.repo_name }}
          body: |
            ## 🤖 Automated Dataset and Model Generation
            
            This PR adds the semantic dataset and trained models generated for **${{ steps.check-dataset.outputs.repo_name }}** repository analysis.
            
            ### 📊 What's included:
            - **Dataset**: `data/${{ steps.check-dataset.outputs.repo_name }}_metrics.csv` - Semantic metrics extracted from the repository
            - **Trained Models**: `seantic_trained_models/*.pkl` - Pre-trained REPD models for bug prediction
            - **Training Scripts**: Enhanced training utilities with serialization fixes
            - **Configuration**: Updated .gitignore and extraction scripts
            
            ### ✅ Ready for semantic analysis
            Once this PR is merged, the GlitchWitcher bot will be able to perform semantic analysis on PRs from the **${{ steps.check-dataset.outputs.repo_name }}** repository without needing to generate datasets or train models each time.
            
            **Generated by**: GlitchWitcher Semantic Workflow
          base: master
          delete-branch: true
          add-paths: |
            data/*.csv
            seantic_trained_models/*.pkl
            train_semantic_model_safe.py
            load_semantic_model.py
            .gitignore
            extract_semantic_dataset.sh

      - name: Notify on PR Creation
        if: steps.check-dataset.outputs.dataset_exists == 'false' || steps.check-dataset.outputs.model_exists == 'false'
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const prNumber = '${{ steps.create-pr-dataset-models.outputs.pull-request-number }}';
            const branchName = 'add-dataset-models-${{ steps.check-dataset.outputs.repo_name }}';
            const repoName = '${{ steps.check-dataset.outputs.repo_name }}';
            
            if (prNumber && prNumber !== '') {
              // PR was created successfully
              const message = `✅ **Dataset and models generated successfully!**
              
              📋 **Created PR #${prNumber}** with dataset and trained models for **${repoName}**
              
              🔗 **Next steps**: 
              - Review and merge PR #${prNumber} to enable semantic analysis
              - Once merged, GlitchWitcher will use these pre-trained models for faster analysis
              
              📊 **What was generated**:
              - Semantic dataset with metrics from ${repoName} repository
              - Pre-trained REPD models for bug prediction
              - Enhanced training utilities with serialization fixes`;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: message
              });
            } else {
              // PR creation failed, provide manual instructions
              const manualUrl = `https://github.com/${context.repo.owner}/${context.repo.repo}/compare/master...${branchName}`;
              const message = `⚠️ **Dataset and models generated, but automatic PR creation failed**
              
              📋 **Generated for**: ${repoName} repository
              📂 **Branch created**: \`${branchName}\`
              
              🔧 **Manual action required**: 
              Please create a PR manually: [Create PR](${manualUrl})
              
              📊 **What was generated**:
              - ✅ Semantic dataset: \`data/${repoName}_metrics.csv\`
              - ✅ Trained models: \`seantic_trained_models/*.pkl\`
              - ✅ Enhanced training utilities
              - ✅ All files pushed to branch \`${branchName}\`
              
              🎯 **Why manual PR needed**: GitHub Actions token doesn't have permission to create PRs automatically in this repository.`;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: message
              });
            }

      - name: Notify on PR Creation Failure (Legacy)
        if: false  # Disabled - using enhanced notification above
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            # This step is now handled by the enhanced notification above

      - name: Use Local Dataset
        run: |
          echo "Setting up dataset for semantic analysis..."
          if [ "${{ steps.check-dataset.outputs.dataset_exists }}" = "true" ]; then
            echo "✅ Using existing local dataset: ${{ steps.check-dataset.outputs.dataset_path }}"
            echo "csv_file_path=${{ steps.check-dataset.outputs.dataset_path }}" >> $GITHUB_ENV
          else
            # Dataset was generated in previous step, use the same path
            dataset_file="${{ steps.check-dataset.outputs.dataset_file }}"
            if [ -f "$dataset_file" ]; then
              echo "✅ Using newly generated dataset: $dataset_file"
              echo "csv_file_path=$dataset_file" >> $GITHUB_ENV
            else
              echo "⚠️ No dataset available, creating minimal sample dataset..."
              # Create minimal sample dataset as fallback
              mkdir -p data
              fallback_file="${{ steps.check-dataset.outputs.dataset_file }}"
              echo "project_name,version,class_name,wmc,rfc,loc,max_cc,avg_cc,cbo,ca,ce,ic,cbm,lcom,lcom3,dit,noc,mfa,npm,dam,moa,cam,amc,bug" > "$fallback_file"
              echo "${{ steps.check-dataset.outputs.repo_name }},unknown,com.example.TestClass,5,10,68,6,2.8,5,0,0,0,1,10,1.0,0,0,0.0,4,0.6666666666666666,2,0.5,13.6,2" >> "$fallback_file"
              echo "Created fallback dataset: $fallback_file"
              echo "csv_file_path=$fallback_file" >> $GITHUB_ENV
            fi
          fi

      - name: Prepare Local Models
        run: |
          echo "Preparing local models for semantic analysis..."
          # Create trained_model directory if it doesn't exist
          mkdir -p trained_model
          
          # Check if local models exist in seantic_trained_models (now should exist after generation/training)
          models_found="false"
          if [ -f "seantic_trained_models/repd_model_DA.pkl" ]; then
            echo "Copying local REPD model..."
            cp seantic_trained_models/repd_model_DA.pkl trained_model/
            models_found="true"
          fi
          
          if [ -f "seantic_trained_models/scaler.pkl" ]; then
            echo "Copying local scaler..."
            cp seantic_trained_models/scaler.pkl trained_model/
            models_found="true"
          fi
          
          if [ -f "seantic_trained_models/training_results.pkl" ]; then
            echo "Copying local training results..."
            cp seantic_trained_models/training_results.pkl trained_model/
            models_found="true"
          fi
          
          if [ "$models_found" = "false" ]; then
            echo "ERROR: No models found after training/generation step"
            exit 1
          else
            echo "Local models prepared successfully"
          fi

      - name: Run Semantic Analysis on Target PR
        id: analysis
        run: |
          set -e
          echo "Running GlitchWitcher semantic analysis on ${{ steps.parse-command.outputs.pr_link }}..."
          
          # Get PR details using GitHub API
          pr_api_url="https://api.github.com/repos/${{ steps.parse-command.outputs.repo_owner }}/${{ steps.parse-command.outputs.repo_name }}/pulls/${{ steps.parse-command.outputs.pr_number }}"
          pr_info=$(curl -s -H "Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}" -H "Accept: application/vnd.github.v3+json" "$pr_api_url")
          base_sha=$(printf "%s" "$pr_info" | python3 -c "import sys, json; d=json.load(sys.stdin); print(d['base']['sha'])")
          head_sha=$(printf "%s" "$pr_info" | python3 -c "import sys, json; d=json.load(sys.stdin); print(d['head']['sha'])")
          echo "Base SHA: $base_sha"
          echo "Head SHA: $head_sha"
          
          # Clone the target repository
          git clone "${{ steps.parse-command.outputs.repo_url }}" target_repo
          # Ensure we have the PR head commit (works for forks)
          git -C target_repo fetch origin "pull/${{ steps.parse-command.outputs.pr_number }}/head:prhead" || true
          git -C target_repo fetch --all --tags --prune
          
          # Get changed files between base..head (Java only)
          merge_base=$(git -C target_repo merge-base "$base_sha" "$head_sha")
          echo "Merge base: $merge_base"
          changed_files=$(git -C target_repo diff --name-only "$merge_base" "$head_sha" | grep -E "\.java$" || true)
          if [ -z "$changed_files" ]; then
            echo "No Java files changed in this PR"
            echo "comment=No Java files found in the PR changes." >> $GITHUB_OUTPUT
            exit 0
          fi
          echo "Changed files:"
          printf "%s\n" "$changed_files"
          
          # Extract semantic features for base commit
          git -C target_repo checkout "$base_sha"
          mkdir -p metrics_output_base
          echo "project_name,version,class_name,wmc,rfc,loc,max_cc,avg_cc,cbo,ca,ce,ic,cbm,lcom,lcom3,dit,noc,mfa,npm,dam,moa,cam,amc,bug" > metrics_output_base/summary_metrics.csv
          
          for file in $changed_files; do
            fpath="target_repo/$file"
            if [ -f "$fpath" ]; then
              echo "Processing $file (base)..."
              # Use the downloaded Java analysis script with error handling
              if python3 analyze_java_file.py "$fpath" "${{ steps.parse-command.outputs.repo_name }}" "unknown" "target_repo"; then
                if [ -f "temp_metrics.csv" ]; then
                  # Check if temp_metrics.csv has data (more than just header)
                  if [ $(wc -l < "temp_metrics.csv") -gt 1 ]; then
                    tail -n +2 "temp_metrics.csv" >> metrics_output_base/summary_metrics.csv
                    echo "✅ Successfully processed $file"
                  else
                    echo "⚠️ No data in temp_metrics.csv for $file"
                  fi
                  rm "temp_metrics.csv"
                fi
              else
                echo "⚠️ Failed to process $file, continuing with next file..."
              fi
            else
              echo "Warning: File not found at $fpath (base)."
            fi
          done
          
          # Extract semantic features for head commit
          git -C target_repo checkout "$head_sha"
          mkdir -p metrics_output_head
          echo "project_name,version,class_name,wmc,rfc,loc,max_cc,avg_cc,cbo,ca,ce,ic,cbm,lcom,lcom3,dit,noc,mfa,npm,dam,moa,cam,amc,bug" > metrics_output_head/summary_metrics.csv
          
          for file in $changed_files; do
            fpath="target_repo/$file"
            if [ -f "$fpath" ]; then
              echo "Processing $file (head)..."
              # Use the downloaded Java analysis script with error handling
              if python3 analyze_java_file.py "$fpath" "${{ steps.parse-command.outputs.repo_name }}" "unknown" "target_repo"; then
                if [ -f "temp_metrics.csv" ]; then
                  # Check if temp_metrics.csv has data (more than just header)
                  if [ $(wc -l < "temp_metrics.csv") -gt 1 ]; then
                    tail -n +2 "temp_metrics.csv" >> metrics_output_head/summary_metrics.csv
                    echo "✅ Successfully processed $file"
                  else
                    echo "⚠️ No data in temp_metrics.csv for $file"
                  fi
                  rm "temp_metrics.csv"
                fi
              else
                echo "⚠️ Failed to process $file, continuing with next file..."
              fi
            else
              echo "Warning: File not found at $fpath (head)."
            fi
          done
          
          echo "=== Row counts ==="
          base_rows=$(wc -l < metrics_output_base/summary_metrics.csv || echo 0)
          head_rows=$(wc -l < metrics_output_head/summary_metrics.csv || echo 0)
          echo "Base rows: $base_rows"
          echo "Head rows: $head_rows"
          
          # Check if we have actual data (more than just headers)
          if [ "$base_rows" -le 1 ] || [ "$head_rows" -le 1 ]; then
            echo "⚠️ Insufficient data for analysis (need more than 1 row per file)"
            echo "comment=Insufficient Java class data extracted for semantic analysis. Only $base_rows base rows and $head_rows head rows found." >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Prepare trained model: Use local models that should now be available
          have_model="false"
          if [ -d "seantic_trained_models" ] && [ -f "seantic_trained_models/repd_model_DA.pkl" ]; then
            echo "Using local semantic model artifacts..."
            have_model="true"
          elif [ -d "trained_model" ] && [ -f "trained_model/repd_model_DA.pkl" ]; then
            echo "Found models in trained_model directory"
            have_model="true"
          else
            echo "ERROR: No trained models found after preparation step"
          fi

          if [ "$have_model" != "true" ]; then
            echo "comment=Unable to train or find semantic model for predictions." >> $GITHUB_OUTPUT
            exit 0
          fi

          # Use the downloaded compare_semantic_predictions.py file
          echo "Running semantic comparison predictions..."
          echo "🔍 Analyzing semantic features and training models..."
          if [ -f "metrics_output_base/summary_metrics.csv" ] && [ -f "metrics_output_head/summary_metrics.csv" ]; then
            # Run the comparison using the downloaded Python file (redirect stderr to suppress progress bars)
            if python3 compare_semantic_predictions.py > semantic_analysis_result.txt 2>/dev/null; then
              # Check if the output file has content
              if [ -s "semantic_analysis_result.txt" ]; then
                echo "✅ Semantic analysis completed successfully"
                # Format the comment with better structure
                {
                  echo "comment<<EOF"
                  echo "## 🔮 GlitchWitcher Analysis Results"
                  echo "**Target PR:** ${{ steps.parse-command.outputs.pr_link }}"
                  echo "**Repository:** ${{ steps.parse-command.outputs.full_repo_name }}"
                  echo ""
                  cat semantic_analysis_result.txt
                  echo ""
                  echo "### 📋 Interpretation Note:"
                  echo "> The values shown are Probability Densities (PDFs), not probabilities. They represent the model's assessment of how likely a file's characteristics are to be 'defective' vs. 'non-defective'. A higher value indicates a better fit for that category. Very small values are expected and normal."
                  echo ""
                  echo "EOF"
                } >> $GITHUB_OUTPUT
              else
                echo "⚠️ Semantic analysis produced no output"
                echo "comment=Semantic comparison prediction produced no output." >> $GITHUB_OUTPUT
              fi
            else
              echo "❌ Error running semantic analysis"
              cat semantic_analysis_result.txt
              echo "comment=Error running semantic comparison predictions." >> $GITHUB_OUTPUT
            fi
          else
            echo "❌ Missing base or head metrics files for semantic comparison"
            echo "comment=Missing base or head metrics files for semantic comparison." >> $GITHUB_OUTPUT
          fi

      - name: Comment on PR
        if: steps.analysis.outputs.comment != ''
        uses: actions/github-script@v6
        env:
          COMMENT_BODY: "${{ steps.analysis.outputs.comment }}"
          PR_LINK: "${{ steps.parse-command.outputs.pr_link }}"
          REPO_NAME: "${{ steps.parse-command.outputs.full_repo_name }}"
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const commentBody = `${process.env.COMMENT_BODY}

            *Analysis performed by GlitchWitcher Bot*`;
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: commentBody
            });